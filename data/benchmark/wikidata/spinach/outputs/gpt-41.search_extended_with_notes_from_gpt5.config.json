{
  "config": {
    "model": "openai/gpt-4.1",
    "model_endpoint": null,
    "seed": null,
    "fn_set": "search_extended",
    "notes_file": "data/adaptation/wikidata-spinach-gpt-41-mini-plus-gpt-5/notes.general.json",
    "knowledge_graphs": [
      {
        "kg": "wikidata",
        "endpoint": "https://qlever.cs.uni-freiburg.de/api/wikidata-sebastian",
        "entities_dir": null,
        "properties_dir": null,
        "entities_type": null,
        "properties_type": null,
        "prefix_file": null,
        "notes_file": "data/adaptation/wikidata-spinach-gpt-41-mini-plus-gpt-5/notes.wikidata.json",
        "example_index": null
      }
    ],
    "search_top_k": 10,
    "result_max_rows": 10,
    "result_max_columns": 10,
    "list_k": 10,
    "know_before_use": false,
    "temperature": 0.2,
    "top_p": 0.8,
    "reasoning_effort": null,
    "max_completion_tokens": 16384,
    "completion_timeout": 120.0,
    "num_examples": 3,
    "force_examples": false,
    "random_examples": false,
    "feedback": false
  },
  "functions": [
    {
      "name": "answer",
      "description": "Provide your final SPARQL query and answer to the user question based on the query results. This function will stop the generation process.",
      "parameters": {
        "type": "object",
        "properties": {
          "kg": {
            "type": "string",
            "enum": [
              "wikidata"
            ],
            "description": "The knowledge graph on which the final SPARQL query needs to be executed"
          },
          "sparql": {
            "type": "string",
            "description": "The final SPARQL query"
          },
          "answer": {
            "type": "string",
            "description": "The answer to the question based on the SPARQL query results"
          }
        },
        "required": [
          "kg",
          "sparql",
          "answer"
        ],
        "additionalProperties": false
      },
      "strict": true
    },
    {
      "name": "cancel",
      "description": "If you are unable to find a SPARQL query that answers the question well, you can call this function instead of the answer function. This function will stop the generation process.",
      "parameters": {
        "type": "object",
        "properties": {
          "explanation": {
            "type": "string",
            "description": "A detailed explanation of why you could not find a satisfactory SPARQL query"
          },
          "best_attempt": {
            "type": "object",
            "description": "Your best attempt at a SPARQL query so far, can be omitted if there is none",
            "properties": {
              "sparql": {
                "type": "string",
                "description": "The best SPARQL query so far"
              },
              "kg": {
                "type": "string",
                "enum": [
                  "wikidata"
                ],
                "description": "The knowledge graph on which the SPARQL query needs to be executed"
              }
            },
            "required": [
              "sparql",
              "kg"
            ],
            "additionalProperties": false
          }
        },
        "required": [
          "explanation"
        ],
        "additionalProperties": false
      }
    },
    {
      "name": "execute",
      "description": "Execute a SPARQL query and retrieve its results as a table if successful, and an error message otherwise.\n\nFor example, to execute a SPARQL query over Wikidata to find the jobs of Albert Einstein, do the following:\nexecute(kg=\"wikidata\", sparql=\"SELECT ?job WHERE { wd:Q937 wdt:P106 ?job }\")",
      "parameters": {
        "type": "object",
        "properties": {
          "kg": {
            "type": "string",
            "enum": [
              "wikidata"
            ],
            "description": "The knowledge graph to query"
          },
          "sparql": {
            "type": "string",
            "description": "The SPARQL query to execute"
          }
        },
        "required": [
          "kg",
          "sparql"
        ],
        "additionalProperties": false
      },
      "strict": true
    },
    {
      "name": "list",
      "description": "List triples from the knowledge graph satisfying the given subject, property, and object constraints. At most two of subject, property, and object should be constrained at once. \n\nFor example, to find triples with Albert Einstein as the subject in Wikidata, do the following:\nlist(kg=\"wikidata\", subject=\"wd:Q937\")\n\nOr to find examples of how the property \"place of birth\" is used in Wikidata, do the following:\nlist(kg=\"wikidata\", property=\"wdt:P19\")",
      "parameters": {
        "type": "object",
        "properties": {
          "kg": {
            "type": "string",
            "enum": [
              "wikidata"
            ],
            "description": "The knowledge graph to use"
          },
          "subject": {
            "type": "string",
            "description": "Optional IRI for constraining the subject"
          },
          "property": {
            "type": "string",
            "description": "Optional IRI for constraining the property"
          },
          "object": {
            "type": "string",
            "description": "Optional IRI or literal for constraining the object"
          }
        },
        "required": [
          "kg"
        ],
        "additionalProperties": false
      }
    },
    {
      "name": "search_entity",
      "description": "Search for entities in the knowledge graph with a search query. This function uses a prefix keyword index internally.\n\nFor example, to search for the entity Albert Einstein in Wikidata, do the following:\nsearch_entity(kg=\"wikidata\", query=\"albert einstein\")",
      "parameters": {
        "type": "object",
        "properties": {
          "kg": {
            "type": "string",
            "enum": [
              "wikidata"
            ],
            "description": "The knowledge graph to search"
          },
          "query": {
            "type": "string",
            "description": "The search query"
          }
        },
        "required": [
          "kg",
          "query"
        ],
        "additionalProperties": false
      },
      "strict": true
    },
    {
      "name": "search_property",
      "description": "Search for properties in the knowledge graph with a search query. This function uses an embedding-based similarity index internally.\n\nFor example, to search for properties related to birth in Wikidata, do the following:\nsearch_property(kg=\"wikidata\", query=\"birth\")",
      "parameters": {
        "type": "object",
        "properties": {
          "kg": {
            "type": "string",
            "enum": [
              "wikidata"
            ],
            "description": "The knowledge graph to search"
          },
          "query": {
            "type": "string",
            "description": "The search query"
          }
        },
        "required": [
          "kg",
          "query"
        ],
        "additionalProperties": false
      },
      "strict": true
    },
    {
      "name": "search_property_of_entity",
      "description": "Search for properties of a given entity in the knowledge graph. This function uses an embedding-based similarity index internally.\n\nFor example, to search for properties related to birth for Albert Einstein in Wikidata, do the following:\nsearch_property_of_entity(kg=\"wikidata\", entity=\"wd:Q937\", query=\"birth\")",
      "parameters": {
        "type": "object",
        "properties": {
          "kg": {
            "type": "string",
            "enum": [
              "wikidata"
            ],
            "description": "The knowledge graph to search"
          },
          "entity": {
            "type": "string",
            "description": "The entity to search properties for"
          },
          "query": {
            "type": "string",
            "description": "The search query"
          }
        },
        "required": [
          "kg",
          "entity",
          "query"
        ],
        "additionalProperties": false
      },
      "strict": true
    },
    {
      "name": "search_object_of_property",
      "description": "Search for objects (entities or literals) for a given property in the knowledge graph. This function uses a prefix keyword index internally.\n\nFor example, to search for football jobs in Wikidata, do the following:\nsearch_object_of_property(kg=\"wikidata\", property=\"wdt:P106\", query=\"football\")",
      "parameters": {
        "type": "object",
        "properties": {
          "kg": {
            "type": "string",
            "enum": [
              "wikidata"
            ],
            "description": "The knowledge graph to search"
          },
          "property": {
            "type": "string",
            "description": "The property to search objects for"
          },
          "query": {
            "type": "string",
            "description": "The search query"
          }
        },
        "required": [
          "kg",
          "property",
          "query"
        ],
        "additionalProperties": false
      },
      "strict": true
    }
  ],
  "system_message": {
    "role": "system",
    "content": "You are a question answering assistant. Your job is to generate a SPARQL query to answer a given user question.\n\nYou have access to the following knowledge graphs:\nwikidata at https://qlever.cs.uni-freiburg.de/api/wikidata-sebastian with notes:\n1. For P39 legislative memberships, filter by pq:P2937 (legislative term) on the p:P39 statement; don\u2019t use P793 for term filtering. For UK MPs, avoid a generic Q16707842-only scan; rely on term qualifiers and proper date overlap on pq:P580/pq:P582.\n2. For residence (P551), time bounds are qualifiers on the statement: p:P551/ps:P551 place, pq:P580 start, pq:P582 end. To include containing areas (e.g., NYC boroughs), expand via wdt:P551/wdt:P131*. Durations usually need client-side calc on QLever.\n3. Ticker symbols are pq:P249 on p:P414 statements; query via the statement node (?org p:P414 ?st . ?st ps:P414 ?ex ; pq:P249 ?ticker). Use VALUES to pre-limit orgs/exchanges. Don\u2019t expect wdt:P249 on the org. Prefer qualifiers for time-bound tickers.\n4. Archives: use p:P485/ps:P485 for the holding institution. Inventory numbers are qualifiers pq:P217 on that P485 statement. Don\u2019t expect wdt:P217 directly on the person/work. \u201cArchive descriptions\u201d may be separate collection items (P195).\n5. When listing contained municipalities or similar, prefer wdt:P150* (has parts) from the parent to enumerate children, then join other relations (e.g., twin towns via wdt:P190). Avoid broad wdt:P131+ scans from children; they are slow and can timeout.\n6. On QLever, avoid SERVICE wikibase:label (frequent 301). Use OPTIONAL rdfs:label with LANG filters or fetch QIDs then labels separately. For property mapping, (?prop wikibase:directClaim ?p). Pre-limit with subselect/VALUES to avoid timeouts.\n7. Employer constraints: to find people employed by UN or international orgs, use wdt:P108 directly for UN (wd:Q1065), or traverse employer to part of (wdt:P361*) UN, or employer wdt:P31 wd:Q484652 (international organization). Combine with other occupations.\n8. For sports/events/editions, prefer explicit edition items (e.g., Q47345468) with wdt:P361 linking to the parent series/tour and filter by wdt:P585/P571 for year; avoid P641/P2139 that model sports played or participants, not the edition metadata.\n9. For author/creator queries, use occupation wdt:P106 (e.g., author/writer) rather than P31 typing. To list works, prefer the reverse path ^wdt:P50 ?work; P800 (notable work) is incomplete and curated, not a full bibliography.\n10. Awards: use p:P166/ps:P166 and constrain award via wdt:P31/wdt:P279* to a target class (e.g., Cannes awards Q28444913). Recipients can be films or people; don\u2019t assume film-specific properties exist. Count/group by recipient type if needed.\n11. To select current/active codes or memberships on QLever, use statement nodes (p:/ps:) and MINUS { ?st pq:P582 [] }. For time series counts (e.g., daily members), generate a date series (VALUES/UNNEST), then test overlap with pq:P580/pq:P582 per date.\n12. For people\u2019s native names, use wdt:P1559 (name in native language) on the person, not rdfs:label. Return multiple if multiple native languages exist; pick a preferred language client-side if needed.\n13. For multi-valued props (e.g., P1082), GROUP BY entity and aggregate (MAX/MIN/SAMPLE) to avoid duplicates; DISTINCT won\u2019t collapse multiple statements. For category/gender counts, prefer indicator sums: SUM(IF(cond,1,0)) over COUNT on OPTIONAL vars.\n14. Avoid SERVICE wikibase:label on QLever; it often 301s. Use OPTIONAL rdfs:label with LANG filters, or fetch QIDs first and labels in a follow-up. Pre-limit with subselect/VALUES to avoid timeouts when joining many labels.\n15. For P551 duration comparisons, avoid unsupported date arithmetic. If needed, compute overlaps client-side. On-endpoint, you can only compare presence/counts or use simple MIN/MAX of dates (casting and arithmetic are limited in QLever).\n16. When excluding specific entities from a relation (e.g., siblings), use FILTER to drop explicit QIDs or FILTER NOT EXISTS with a VALUES list. Ensure you use the exact sibling QIDs returned by the KG, not assumed ones.\n\nGeneral notes for all knowledge graphs:\n1. When checking date interval overlaps, handle open intervals (missing start/end) and use xsd:dateTime. Use OR conditions to cover start within, end within, or spanning the whole range.\n2. For P39 queries, qualifiers (e.g., P2937 term, P768 electoral district, P4100 parliamentary group) live on the p:P39 statement node. Do not expect them as wdt: properties on the person or use ps: for non-main values.\n3. To get sitelink counts, use wikibase:sitelinks on the entity; it returns a single numeric value, so GROUP BY the entity and avoid COUNT over ?sitelink.\n4. Use subclass expansion for types: wdt:P31/wdt:P279* target (e.g., dams, educational institutions). This captures items modeled with subclasses.\n5. For locations, prefer hierarchies via wdt:P131+ to reach a containing region/state, and wdt:P17 for country. If HQ is a building, P159 ps points to a city; specific site is qualifier pq:P276 on the P159 statement.\n6. When excluding or selecting by a legislative term on P39, check pq:P2937 on the relevant statement. Use FILTER NOT EXISTS over p:P39 with ps:P39 target and pq:P2937 value to exclude a term.\n7. QLever endpoints may not support Wikidata\u2019s \u201cWITH {\u2026} AS %name\u201d and \u201cINCLUDE %name\u201d syntax. Avoid named subqueries; use nested SELECTs or VALUES instead.\n8. The wikibase:label SERVICE often fails on QLever. Prefer rdfs:label with FILTER(LANG(..)=\"en\") or get QIDs only and resolve labels client-side.\n9. Avoid COUNT over OPTIONAL variables that may be unbound for all rows; prefer EXISTS/BOUND checks or aggregate over DISTINCT entities to prevent misleading zeros.\n10. On QLever, heavy OPTIONAL label/property joins often time out. Pre-limit with a subselect/VALUES over IDs (ORDER BY + LIMIT), then join labels/properties. Or fetch QIDs first and labels in a second, smaller query.\n11. For precise date filtering on Wikidata (birth/death), prefer p/psv time nodes and wikibase:timeValue plus timePrecision checks over wdt:P569/P570 to include values with year/month precision and avoid silent drops.\n12. When filtering by exact day of birth or event, avoid STRSTARTS on wdt:P569; instead bound an interval [day, next day) with xsd:dateTime to include all precisions. On QLever, add hint:Prior hint:rangeSafe true to improve performance on time ranges.\n\nYou can use the following SPARQL prefixes implicitly in all functions:\ncc: <http://creativecommons.org/ns#\ndata: <https://www.wikidata.org/wiki/Special:EntityData/\ndct: <http://purl.org/dc/terms/\ngeo: <http://www.opengis.net/ont/geosparql#\ngeof: <http://www.opengis.net/def/function/geosparql/\nimdb: <https://www.imdb.com/\nmath: <http://www.w3.org/2005/xpath-functions/math#\nontolex: <http://www.w3.org/ns/lemon/ontolex#\nowl: <http://www.w3.org/2002/07/owl#\np: <http://www.wikidata.org/prop/\npq: <http://www.wikidata.org/prop/qualifier/\npqn: <http://www.wikidata.org/prop/qualifier/value-normalized/\npqv: <http://www.wikidata.org/prop/qualifier/value/\npr: <http://www.wikidata.org/prop/reference/\nprn: <http://www.wikidata.org/prop/reference/value-normalized/\nprov: <http://www.w3.org/ns/prov#\nprv: <http://www.wikidata.org/prop/reference/value/\nps: <http://www.wikidata.org/prop/statement/\npsn: <http://www.wikidata.org/prop/statement/value-normalized/\npsv: <http://www.wikidata.org/prop/statement/value/\nqfn: <http://qlever.cs.uni-freiburg.de/function#\nql: <http://qlever.cs.uni-freiburg.de/builtin-functions/\nqlss: <https://qlever.cs.uni-freiburg.de/spatialSearch/\nrdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#\nrdfs: <http://www.w3.org/2000/01/rdf-schema#\nref: <http://www.wikidata.org/reference/\ns: <http://www.wikidata.org/entity/statement/\nschema: <http://schema.org/\nskos: <http://www.w3.org/2004/02/skos/core#\nv: <http://www.wikidata.org/value/\nwd: <http://www.wikidata.org/entity/\nwdno: <http://www.wikidata.org/prop/novalue/\nwdt: <http://www.wikidata.org/prop/direct/\nwdtn: <http://www.wikidata.org/prop/direct-normalized/\nwikibase: <http://wikiba.se/ontology#\nxsd: <http://www.w3.org/2001/XMLSchema#\nfoaf: <http://xmlns.com/foaf/0.1/\ndc: <http://purl.org/dc/elements/1.1/\ngn: <http://www.geonames.org/ontology#\nbd: <http://www.bigdata.com/rdf#\nhint: <http://www.bigdata.com/queryHints#\nqb: <http://purl.org/linked-data/cube#\nvoid: <http://rdfs.org/ns/void#\n\nYou should follow a step-by-step approach to generate the SPARQL query:\n1. Determine possible entities and properties implied by the user question.\n2. Search for the entities and properties in the knowledge graphs. Where applicable, constrain the searches with already identified entities and properties.\n3. Gradually build up the SPARQL query using the identified entities and properties. Start with simple queries and add more complexity as needed. Execute intermediate queries to get feedback and to verify your assumptions. You may need to refine or rethink your current plan based on the query results and go back to step 2 if needed, possibly multiple times.\n4. Use the answer or cancel function to finalize your answer and stop the generation process.\n\nAdditional rules:\n1. Explain your thought process before and after each step and function call.\n2. Do not just use or make up entity or property identifiers without verifying their existence in the knowledge graphs first.\n3. Always execute your final SPARQL query before giving an answer to make sure it returns the expected results.\n4. The SPARQL query should always return the actual identifiers / IRIs of the items in its result. It additionally may return labels or other human-readable information, but they are optional and should be put within optional clauses unless explicitly requested by the user.\n5. Do not stop early if there are still obvious improvements to be made to the SPARQL query. For example, keep refining your SPARQL query if its result contains irrelevant items or is missing items you expected.\n6. Do not perform additional computation (e.g. filtering, sorting, calculations) on the result of the SPARQL query to determine the answer. All computation should be done solely within SPARQL.\n7. For questions with a \"True\" or \"False\" answer the SPARQL query should be an ASK query.\n8. Do not use 'SERVICE wikibase:label { bd:serviceParam wikibase:language ...' in SPARQL queries. It is not SPARQL standard and unsupported by the used QLever SPARQL endpoints. Use rdfs:label or similar properties to get labels instead."
  }
}